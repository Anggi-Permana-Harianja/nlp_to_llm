{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0ea5c0-beff-4e25-a503-75c3145b84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01611ef-a1c2-477c-aa16-c496746af1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.7782\n",
      "Epoch [11/100], Loss: 3.7021\n",
      "Epoch [21/100], Loss: 3.6275\n",
      "Epoch [31/100], Loss: 3.5542\n",
      "Epoch [41/100], Loss: 3.4820\n",
      "Epoch [51/100], Loss: 3.4110\n",
      "Epoch [61/100], Loss: 3.3410\n",
      "Epoch [71/100], Loss: 3.2718\n",
      "Epoch [81/100], Loss: 3.2036\n",
      "Epoch [91/100], Loss: 3.1360\n"
     ]
    }
   ],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Parameter(torch.randn(vocab_size, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.embedding)\n",
    "\n",
    "# samlpe simple linear layer\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_layer = CustomEmbeddingLayer(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding_layer(x)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "# define parameters\n",
    "## vocabs taken from BPE we coded in /tokenizer/BPE.ipynb\n",
    "vocabs = {'w': 0, 'f': 1, 's</w>': 2, 'd': 3, 'h': 4, 'the': 5, 'c': 6, 'a': 7, 'r': 8, 'o': 9, 'k': 10, 'in': 11, 'u': 12, 'b': 13, '.</w>': 14, 'n': 15, 'l': 16, 'e</w>': 17, 'e': 18, 'g': 19, 'x': 20, '-': 21, 'an': 22, 'he': 23, 'm': 24, '</w>': 25, ',</w>': 26, 'p': 27, 'v': 28, 'i': 29, 'd</w>': 30, 'y': 31, 's': 32, 't': 33, 'er': 34}\n",
    "vocab_size = len(vocabs)\n",
    "embed_dim = 4\n",
    "\n",
    "# iiitiate model\n",
    "model = SimpleModel(vocab_size, embed_dim)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# sample training data: batch of 2 sequences, each with 4 token indices\n",
    "# for simplicity, using one-hot encoded inputs\n",
    "sample_input = torch.eye(vocab_size)[[1, 2, 3, 4, 4, 3, 2, 1]].reshape(2, 4, vocab_size)\n",
    "sample_target = torch.tensor([[2, 3, 4, 5], [5, 4, 3, 2]], dtype=torch.long)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(sample_input)\n",
    "    # compute loss\n",
    "    loss = criterion(output.view(-1, vocab_size), sample_target.view(-1))\n",
    "    # backward and optimizer\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a304937e-0c9e-43da-8497-0719f2295a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Embeddings: tensor([[-1.0888, -0.2824,  0.4090,  0.6687],\n",
      "        [ 0.9902,  0.1060, -0.1460,  0.9678],\n",
      "        [ 1.1455,  0.5316, -0.0126,  0.1891],\n",
      "        [-0.4129, -0.5683,  0.2142, -0.6977],\n",
      "        [-1.2059,  0.6726, -0.5572,  0.1176],\n",
      "        [ 0.7144, -2.4413,  0.4394,  1.0270],\n",
      "        [-1.6094, -0.5242,  0.4525, -0.1211],\n",
      "        [-0.1593, -0.2036,  0.9511, -0.4252],\n",
      "        [ 0.7152, -0.6197,  0.7950,  1.1820],\n",
      "        [-0.0077, -1.6696, -1.3080,  0.3963],\n",
      "        [-0.8808,  0.1096,  1.1021,  2.3211],\n",
      "        [-1.5365,  1.3074,  2.4265, -1.3851],\n",
      "        [-0.3820,  2.0571,  0.5493, -0.5722],\n",
      "        [ 1.8188,  0.0133, -0.3563, -0.2530],\n",
      "        [ 1.5999,  0.5760,  0.3622, -1.4171],\n",
      "        [ 1.0916, -1.0916, -0.5442, -0.4348],\n",
      "        [ 1.9268, -0.1019, -0.1726, -0.9178],\n",
      "        [ 0.2730,  0.1792, -0.5092, -0.1255],\n",
      "        [-1.0621,  0.8124, -1.2130, -1.7982],\n",
      "        [ 0.5199,  0.4318, -0.6762, -0.2020],\n",
      "        [-1.1380, -0.7472,  0.6408,  0.0838],\n",
      "        [-1.0337, -1.3030,  0.4298, -1.6550],\n",
      "        [ 0.3106, -0.7349, -0.2298,  0.7607],\n",
      "        [-0.3178, -0.0850, -0.1859,  1.2961],\n",
      "        [ 0.5256,  1.7584,  0.6722,  1.9115],\n",
      "        [ 0.9362,  1.6854, -0.3124, -0.2484],\n",
      "        [ 1.7544,  0.4660, -0.1214,  0.8796],\n",
      "        [ 0.0111,  2.6949, -0.9221, -0.2964],\n",
      "        [ 0.3798, -0.4846, -0.6751, -0.8592],\n",
      "        [ 0.0222, -0.0067,  0.9053,  0.8688],\n",
      "        [ 1.1010,  0.1080,  0.7099,  1.1744],\n",
      "        [ 0.9672, -1.9824,  1.5767,  0.4583],\n",
      "        [ 0.9511,  1.5553,  1.3151, -0.3592],\n",
      "        [-0.1451, -0.7237,  0.5969, -1.1076],\n",
      "        [ 0.2883,  0.6982, -0.0380, -0.0168]])\n",
      "Learned Embedding size: torch.Size([35, 4])\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings = model.embedding_layer.embedding.data\n",
    "print(\"Learned Embeddings:\", learned_embeddings)\n",
    "print(f\"Learned Embedding size: {learned_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa58750b-0612-45f3-9f0b-4859c706f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'w': tensor([-1.0888, -0.2824,  0.4090,  0.6687])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding for 'w': {learned_embeddings[vocabs['w']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e83fa93-e5e2-46d0-a645-7b26fd645a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'the': tensor([ 0.7144, -2.4413,  0.4394,  1.0270])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding for 'the': {learned_embeddings[vocabs['the']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3d350-ae05-490a-a56f-87d3f9ff5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
