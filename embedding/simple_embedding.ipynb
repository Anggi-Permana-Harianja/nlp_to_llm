{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0ea5c0-beff-4e25-a503-75c3145b84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01611ef-a1c2-477c-aa16-c496746af1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.7846\n",
      "Epoch [11/100], Loss: 3.6986\n",
      "Epoch [21/100], Loss: 3.6135\n",
      "Epoch [31/100], Loss: 3.5292\n",
      "Epoch [41/100], Loss: 3.4458\n",
      "Epoch [51/100], Loss: 3.3631\n",
      "Epoch [61/100], Loss: 3.2810\n",
      "Epoch [71/100], Loss: 3.1996\n",
      "Epoch [81/100], Loss: 3.1187\n",
      "Epoch [91/100], Loss: 3.0385\n"
     ]
    }
   ],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Parameter(torch.randn(vocab_size, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.embedding)\n",
    "\n",
    "# samlpe simple linear layer\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_layer = CustomEmbeddingLayer(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding_layer(x)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "# define parameters\n",
    "## vocabs taken from BPE we coded in /tokenizer/BPE.ipynb\n",
    "vocabs = {'w': 0, 'f': 1, 's</w>': 2, 'd': 3, 'h': 4, 'the': 5, 'c': 6, 'a': 7, 'r': 8, 'o': 9, 'k': 10, 'in': 11, 'u': 12, 'b': 13, '.</w>': 14, 'n': 15, 'l': 16, 'e</w>': 17, 'e': 18, 'g': 19, 'x': 20, '-': 21, 'an': 22, 'he': 23, 'm': 24, '</w>': 25, ',</w>': 26, 'p': 27, 'v': 28, 'i': 29, 'd</w>': 30, 'y': 31, 's': 32, 't': 33, 'er': 34}\n",
    "vocab_size = len(vocabs)\n",
    "embed_dim = 3\n",
    "\n",
    "# iiitiate model\n",
    "model = SimpleModel(vocab_size, embed_dim)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# sample training data: batch of 2 sequences, each with 4 token indices\n",
    "# for simplicity, using one-hot encoded inputs\n",
    "sample_input = torch.eye(vocab_size)[[1, 2, 3, 4, 4, 3, 2, 1]].reshape(2, 4, vocab_size)\n",
    "sample_target = torch.tensor([[2, 3, 4, 5], [5, 4, 3, 2]], dtype=torch.long)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(sample_input)\n",
    "    # compute loss\n",
    "    loss = criterion(output.view(-1, vocab_size), sample_target.view(-1))\n",
    "    # backward and optimizer\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a304937e-0c9e-43da-8497-0719f2295a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Embeddings: tensor([[ 1.1371,  0.9943,  0.0375],\n",
      "        [-1.6018, -0.3988, -0.9190],\n",
      "        [ 0.5137,  1.5477,  0.5030],\n",
      "        [-0.4879, -0.9742, -0.0280],\n",
      "        [ 0.2598,  1.1345,  0.4362],\n",
      "        [ 1.4705, -0.6515,  0.0917],\n",
      "        [-0.0590, -1.2082,  0.1376],\n",
      "        [ 0.1255,  0.7336, -1.5637],\n",
      "        [-0.2186,  0.9550,  1.5034],\n",
      "        [ 1.6238, -1.2249, -1.6470],\n",
      "        [ 0.4653,  0.1373, -2.0911],\n",
      "        [-2.1927,  0.7636,  0.2861],\n",
      "        [ 0.6090, -0.2787, -0.2165],\n",
      "        [ 0.2153, -0.9786, -0.9581],\n",
      "        [-0.3860, -0.5954, -0.8105],\n",
      "        [-0.8320, -1.4884,  0.5985],\n",
      "        [ 0.7002,  0.6555,  0.8853],\n",
      "        [ 0.0209, -0.3941,  0.8519],\n",
      "        [ 0.7330,  1.8007,  0.9618],\n",
      "        [ 0.1652, -0.8729, -0.4493],\n",
      "        [ 2.4194,  0.1464, -0.9133],\n",
      "        [ 0.4371, -0.5891, -0.4020],\n",
      "        [ 1.4615,  1.9300,  1.3794],\n",
      "        [-1.3398,  1.9100,  0.1663],\n",
      "        [ 0.4614,  0.6157, -0.3563],\n",
      "        [ 1.4832,  0.1718, -1.1843],\n",
      "        [ 1.0340,  0.2075,  0.1639],\n",
      "        [-0.5152, -0.0870, -0.7061],\n",
      "        [ 1.2986,  0.3155, -1.2322],\n",
      "        [-0.0044,  0.8870,  1.1209],\n",
      "        [ 0.3891, -0.7595, -2.1774],\n",
      "        [-1.0783,  0.1202,  1.8131],\n",
      "        [ 2.0008, -0.5291, -1.4354],\n",
      "        [ 0.9304,  1.6046, -0.1905],\n",
      "        [ 0.3064, -2.1957, -1.9596]])\n",
      "Learned Embedding size: torch.Size([35, 3])\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings = model.embedding_layer.embedding.data\n",
    "print(\"Learned Embeddings:\", learned_embeddings)\n",
    "print(f\"Learned Embedding size: {learned_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa58750b-0612-45f3-9f0b-4859c706f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'w': tensor([1.1371, 0.9943, 0.0375])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding for 'w': {learned_embeddings[vocabs['w']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e83fa93-e5e2-46d0-a645-7b26fd645a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'the': tensor([ 1.4705, -0.6515,  0.0917])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding for 'the': {learned_embeddings[vocabs['the']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3d350-ae05-490a-a56f-87d3f9ff5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
