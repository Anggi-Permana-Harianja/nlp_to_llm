{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "226551aa-7db0-41ce-a6c1-e4683f4f6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3018df83-cd2a-41ad-ae2e-89504bd5baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0 # d_model must be divisible by num heads\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # initialize weight for Q, K, V\n",
    "        self.WQ = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.WK = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.WV = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.dense = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        split the last dimension into (num_heads, depth)\n",
    "        Transpose the result to shape (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return np.transpose(x, (0, 2, 1, 3))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "        return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "\n",
    "        # softmax\n",
    "        attention_weights = self.softmax(scores)\n",
    "        output = np.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # linear projections\n",
    "        Q = np.matmul(Q, self.WQ)\n",
    "        K = np.matmul(K, self.WK)\n",
    "        V = np.matmul(V, self.WV)\n",
    "\n",
    "        # split projections into multiple heads\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "\n",
    "        # scaled dot product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # concat heads\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # final layer\n",
    "        output = np.matmul(attention_output, self.dense)\n",
    "\n",
    "        return output, attention_weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "707bb694-8839-46cd-8844-7b5764db7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embedding = torch.tensor([[-1.0888,  0.7176,  0.4090,  1.6687],\n",
    "        [ 1.8317,  0.6463, -0.1360,  1.9677],\n",
    "        [ 2.0548,  0.1155,  0.0074,  1.1889],\n",
    "        [-0.2718, -1.5583,  0.2442,  0.3019],\n",
    "        [-1.9627,  0.0190, -0.5172,  1.1168],\n",
    "        [-0.2445, -2.1576,  0.4894,  2.0258],\n",
    "        [-1.8888,  0.4360,  0.5125,  0.8771],\n",
    "        [ 0.4977,  0.5503,  1.0210,  0.5724],\n",
    "        [ 1.7046, -0.7652,  0.8749,  2.1788],\n",
    "        [ 0.4044, -2.5807, -1.2181,  1.3923],\n",
    "        [-1.4248, -0.7295,  1.2019,  3.3161],\n",
    "        [-2.5365,  1.3118,  2.5363, -0.3911],\n",
    "        [-0.9186,  2.9010,  0.6690,  0.4206],\n",
    "        [ 2.2390,  0.9207, -0.2267,  0.7386],\n",
    "        [ 2.5905,  0.7127,  0.5017, -0.4269],\n",
    "        [ 1.7419, -1.8513, -0.3948,  0.5540],\n",
    "        [ 1.6389, -1.0596, -0.0133,  0.0694],\n",
    "        [-0.6884, -0.0960, -0.3400,  0.8601],\n",
    "        [-1.8131,  1.4727, -1.0340, -0.8144],\n",
    "        [ 0.6698,  1.4205, -0.4873,  0.7800],\n",
    "        [-0.2251, -0.3391,  0.8395,  1.0639],\n",
    "        [-0.1970, -1.8507,  0.6383, -0.6770],\n",
    "        [ 0.3017, -1.7349, -0.0116,  1.7366],\n",
    "        [-1.1640, -0.6178,  0.0421,  2.2698],\n",
    "        [-0.3800,  2.1826,  0.9099,  2.8828],\n",
    "        [ 0.8038,  2.6766, -0.0650,  0.7205],\n",
    "        [ 2.5170,  1.1129,  0.1357,  1.8460],\n",
    "        [ 0.9675,  2.4028, -0.6554,  0.6674],\n",
    "        [ 0.6507, -1.4472, -0.3987,  0.1019],\n",
    "        [-0.6414, -0.7548,  1.1913,  1.8270],\n",
    "        [ 0.1130,  0.2623,  1.0054,  2.1297],\n",
    "        [ 0.5632, -1.0677,  1.8818,  1.4106],\n",
    "        [ 1.5025,  2.3895,  1.6297,  0.5900],\n",
    "        [ 0.8548, -0.7370,  0.9209, -0.1616],\n",
    "        [ 0.8174, -0.1504,  0.2955,  0.9260]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63eb2527-e599-44fe-a84a-ca4e240baec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_size = 1\n",
    "num_heads = 2\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "534eafc9-1792-4f74-99fa-4a6f3bf2880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = combined_embedding\n",
    "K = combined_embedding\n",
    "V = combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a42ea13e-67e0-43f7-8ff2-d1d3a73aaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiHeadAttention(d_model, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "502cda80-efcb-488c-8a22-d548234c98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attention_weights = multihead_attention.forward(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ac4305c-f133-43f8-8d0b-02dbe75f9abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.9593e-05,  2.9258e-04, -6.4020e-04, -6.8415e-04]],\n",
       "\n",
       "        [[ 2.3448e-04,  2.3441e-04, -5.8210e-04, -3.4590e-04]],\n",
       "\n",
       "        [[ 1.3601e-04,  1.0283e-04, -2.2347e-04,  8.2476e-06]],\n",
       "\n",
       "        [[-3.8836e-04,  2.4034e-04,  3.3301e-04,  4.4886e-04]],\n",
       "\n",
       "        [[-1.4968e-04,  4.1858e-04, -4.2946e-04, -4.6737e-04]],\n",
       "\n",
       "        [[-5.4662e-04,  6.4713e-04,  6.9862e-05,  3.4845e-04]],\n",
       "\n",
       "        [[-1.9356e-06,  1.9957e-04, -4.0025e-04, -5.4507e-04]],\n",
       "\n",
       "        [[ 1.9391e-04, -8.9977e-05, -1.9038e-04, -2.1663e-04]],\n",
       "\n",
       "        [[-7.9263e-05,  3.4389e-04, -2.0217e-04,  9.6955e-05]],\n",
       "\n",
       "        [[-6.6544e-04,  7.1029e-04,  2.5435e-04,  6.7115e-04]],\n",
       "\n",
       "        [[-2.6390e-04,  7.4743e-04, -6.4283e-04, -5.1921e-04]],\n",
       "\n",
       "        [[ 2.6520e-04, -3.5438e-04, -1.9668e-04, -6.7417e-04]],\n",
       "\n",
       "        [[ 6.6541e-04, -2.7134e-04, -8.6526e-04, -1.1830e-03]],\n",
       "\n",
       "        [[ 3.3939e-04, -7.4453e-05, -3.2604e-04, -1.6598e-04]],\n",
       "\n",
       "        [[ 3.5440e-04, -3.9926e-04,  9.5039e-05,  1.7106e-04]],\n",
       "\n",
       "        [[-3.6495e-04,  2.6812e-04,  4.0633e-04,  7.5432e-04]],\n",
       "\n",
       "        [[-1.5843e-04,  3.5848e-05,  3.4459e-04,  5.6165e-04]],\n",
       "\n",
       "        [[-8.9933e-05,  2.7554e-04, -2.5423e-04, -2.1560e-04]],\n",
       "\n",
       "        [[ 2.1857e-04, -1.1539e-04, -3.4659e-04, -6.0360e-04]],\n",
       "\n",
       "        [[ 3.5471e-04,  5.6198e-06, -5.6847e-04, -5.4978e-04]],\n",
       "\n",
       "        [[-7.8807e-05,  1.8705e-04, -1.3429e-04, -9.5144e-05]],\n",
       "\n",
       "        [[-4.2583e-04,  1.9988e-05,  6.9028e-04,  7.4144e-04]],\n",
       "\n",
       "        [[-4.2620e-04,  5.5545e-04,  2.6314e-05,  3.1960e-04]],\n",
       "\n",
       "        [[-2.5032e-04,  6.2430e-04, -4.7453e-04, -3.4846e-04]],\n",
       "\n",
       "        [[ 4.9878e-04,  2.7282e-04, -1.2584e-03, -1.3109e-03]],\n",
       "\n",
       "        [[ 6.8348e-04, -2.1218e-04, -8.4414e-04, -9.4828e-04]],\n",
       "\n",
       "        [[ 4.0088e-04,  7.8410e-05, -6.1743e-04, -3.9291e-04]],\n",
       "\n",
       "        [[ 6.0554e-04, -1.3613e-04, -7.9248e-04, -8.3079e-04]],\n",
       "\n",
       "        [[-3.2713e-04,  1.9587e-04,  3.5791e-04,  5.5745e-04]],\n",
       "\n",
       "        [[-2.0198e-04,  3.8605e-04, -2.1685e-04, -1.3988e-04]],\n",
       "\n",
       "        [[ 7.8429e-05,  3.0013e-04, -5.3272e-04, -4.4985e-04]],\n",
       "\n",
       "        [[-1.7262e-04,  1.8067e-04,  8.3458e-05,  2.0624e-04]],\n",
       "\n",
       "        [[ 7.2246e-04, -4.3832e-04, -5.7863e-04, -7.1552e-04]],\n",
       "\n",
       "        [[-8.9263e-05, -1.0367e-04,  3.4411e-04,  4.0357e-04]],\n",
       "\n",
       "        [[ 1.1332e-05,  1.2769e-04, -1.3208e-04, -7.8827e-06]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01d92e8e-367d-47cf-9df7-d993a382376f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 1, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4122c941-0899-4cdd-a0be-b202edb143e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]],\n",
       "\n",
       "         [[1.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d46b12a-0286-4b7e-9fef-d9bebb8e4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 2, 1, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
