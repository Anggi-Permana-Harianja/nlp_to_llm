{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c17df0-0dbc-43d7-8860-8d17071b20fc",
   "metadata": {},
   "source": [
    "# Basic tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c68a21c-8745-45a7-981f-0c191234c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a8c861-785a-45b1-b5bb-fb39720be31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'tokenizer', 'example']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a simple tokenizer example\"\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12729b7-9c46-4100-911e-3f7b3c36ccae",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Tokenizer\n",
    "\n",
    "Byte Pair Encoding is a subword tokenization algorithm. \n",
    "It iteratively merges the most frequent pairs of characters or subwords to create new subwords.\n",
    "\n",
    "**Tokenizers Used by GPT-4**\n",
    "\n",
    "GPT-4, like its predecessors, typically uses a tokenizer based on the Byte Pair Encoding (BPE) method. Hereâ€™s how it generally works:\n",
    "\n",
    "    - Initialization: The tokenizer is initialized with a vocabulary that includes common subwords and characters.\n",
    "    \n",
    "    - Tokenization Process:\n",
    "        \n",
    "        - Input Text: The input text is first split into basic units (characters or initial subwords).\n",
    "        \n",
    "        - Merging: Frequent pairs of characters or subwords are iteratively merged based on their frequency in the training corpus.\n",
    "        \n",
    "        - Final Tokens: The merging process continues until the entire text is converted into a sequence of subwords (tokens) from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f7dda5-cfb7-47de-b2fa-e6cd2c11e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "\n",
    "    def get_vocab(self, text):\n",
    "        \"\"\"\n",
    "        Converts the input text into tokens and counts their frequencies.\n",
    "        \"\"\"\n",
    "        tokens = [tuple(word) + ('</w>', ) for word in text.split()]\n",
    "        vocab = Counter(tokens)\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"\n",
    "        Calculates the frequency of each character pair in the vocabulary.\n",
    "        \"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pairs[(word[i], word[i+1])] += freq\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"\n",
    "        Merges the most frequent pair into a new subword.\n",
    "        \"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = ' '.join(word)\n",
    "            w_out = p.sub(''.join(pair), w_out)\n",
    "            new_vocab[tuple(w_out.split())] = vocab[word]\n",
    "\n",
    "        return new_vocab\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Iteratively merges pairs until the vocabulary reaches the desired size.\n",
    "        \"\"\"\n",
    "        vocab = self.get_vocab(text)\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.vocab[best] = pairs[best]\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        \"\"\"\n",
    "        Tokenizes a new word based on the learned subwords.\n",
    "        \"\"\"\n",
    "        tokens = list(word)\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i + 1])\n",
    "            if bigram in self.vocab:\n",
    "                tokens[i:i+2] = [''.join(bigram)]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f3a122-5476-4aa8-a7a5-07f01f5f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a simple BPE tokenizer example\"\n",
    "bpe_tokenizer = BPE(vocab_size=10)\n",
    "bpe_tokenizer.fit(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5dd362-0680-40b7-a801-d4fcccc028e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'x', 'a', 'mple']\n",
      "['si', 'mple']\n"
     ]
    }
   ],
   "source": [
    "tokens = bpe_tokenizer.tokenize(\"example\")\n",
    "print(tokens)\n",
    "tokens = bpe_tokenizer.tokenize(\"simple\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea61bae0-1dac-4973-ade3-26842f6f15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th', 'is']\n",
      "['is']\n"
     ]
    }
   ],
   "source": [
    "tokens = bpe_tokenizer.tokenize(\"this\")\n",
    "print(tokens)\n",
    "tokens = bpe_tokenizer.tokenize(\"is\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2743d-96c2-4ad1-9bdd-6299b5f04ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
