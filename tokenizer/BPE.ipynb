{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8130e24b-d084-4271-9770-437a88f378aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b995f6-7e58-4ce9-8d55-d3e6e12d7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self, num_merges):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.bpe_ranks = {}\n",
    "        self.inverse_vocab = {}\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in corpus.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, corpus):\n",
    "        new_corpus = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in corpus:\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            new_corpus[new_word] = corpus[word]\n",
    "        return new_corpus\n",
    "\n",
    "    def fit(self, texts):\n",
    "        # Build initial corpus with space-separated characters\n",
    "        corpus = {' '.join(list(word)) + ' </w>': freq for word, freq in Counter(texts).items()}\n",
    "\n",
    "        # Perform num_merges merges\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            corpus = self.merge_vocab(best, corpus)\n",
    "            self.bpe_ranks[best] = i\n",
    "\n",
    "        # Build vocabulary with indices (ENSURE ALL UNIQUE TOKENS)\n",
    "        unique_tokens = set()\n",
    "        for word in corpus.keys():\n",
    "            unique_tokens.update(word.split())\n",
    "        # for encode\n",
    "        self.vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "        # for decode\n",
    "        self.inverse_vocab = {idx: token for token, idx in self.vocab.items()}\n",
    "\n",
    "    def encode(self, word):\n",
    "        word = ' '.join(list(word)) + ' </w>'\n",
    "        while True:\n",
    "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
    "            pairs = [(p[0], p[1]) for p in pairs if (p[0], p[1]) in self.bpe_ranks]\n",
    "            if not pairs:\n",
    "                break\n",
    "            pair = min(pairs, key=lambda pair: self.bpe_ranks[pair])\n",
    "            word = word.replace(' '.join(pair), ''.join(pair))\n",
    "        return word.split()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_text = self.encode(text)\n",
    "        # Handle the case where encoded subwords are not in the vocab\n",
    "        tokenized = []\n",
    "        for subword in encoded_text:\n",
    "            if subword in self.vocab:\n",
    "                tokenized.append(self.vocab[subword])\n",
    "            else:\n",
    "                # Adding unknown token handling (optional)\n",
    "                print(f\"Warning: subword '{subword}' not found in vocabulary.\")\n",
    "        return tokenized\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        subwords = [self.inverse_vocab[token] for token in tokens if token in self.inverse_vocab]\n",
    "        return ''.join(subwords).replace('</w>', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1071331d-466b-4ca2-80c7-480d6e4af0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "In a distant land, there was a village surrounded by lush forests and towering mountains. The villagers lived in harmony with nature, cultivating their fields and tending to their animals. Each day, the sun rose over the mountains, casting a golden glow across the village. Birds sang sweet melodies, and the air was filled with the scent of blooming flowers.\n",
    "\n",
    "In this village, there was a young girl named Lily. She had a curious mind and a kind heart. Every morning, she would explore the forest, discovering new plants and animals. She dreamed of becoming a great healer, using the gifts of nature to help those in need. Her grandmother, who was the village healer, taught her about the medicinal properties of various herbs and plants.\n",
    "\n",
    "One day, a stranger arrived in the village. He was a traveler from a far-off land, seeking knowledge and adventure. Lily was fascinated by his stories and eagerly listened to his tales of distant places and incredible feats. The traveler shared his wisdom with the villagers, teaching them new ways to cultivate their land and improve their lives.\n",
    "\n",
    "As the seasons changed, the village continued to thrive. Lily grew up to become a respected healer, and the traveler became a cherished friend to all. Together, they showed that with knowledge, kindness, and a little bit of curiosity, anything is possible.\n",
    "\"\"\"\n",
    "# Preprocess the text to create a list of words\n",
    "texts = sample_text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb998123-08bc-479e-8e92-5c9dd974ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'s': 0, 'w': 1, 'd</w>': 2, 'o': 3, 'y': 4, 'm': 5, 'an': 6, 'k': 7, 'i': 8, 's</w>': 9, 'n': 10, '</w>': 11, 'v': 12, '.</w>': 13, 'h': 14, 'c': 15, 'd': 16, 'e</w>': 17, 'x': 18, '-': 19, 'p': 20, 'e': 21, 'f': 22, 'g': 23, 'in': 24, 'a': 25, 'he': 26, 'the': 27, ',</w>': 28, 'er': 29, 'r': 30, 't': 31, 'l': 32, 'u': 33, 'b': 34}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "bpe = BytePairEncoding(num_merges=10)\n",
    "bpe.fit(texts)\n",
    "\n",
    "print(\"Vocabulary:\", bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bf7d77c-fcea-4ac5-99e5-7cf9eba6bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'bell': ['b', 'e', 'l', 'l', '</w>']\n",
      "Tokenized 'bell': [34, 21, 32, 32, 11]\n",
      "Decoded '[34, 21, 32, 32, 11]: bell'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new text\n",
    "new_text = \"bell\"\n",
    "encoded_text = bpe.encode(new_text)\n",
    "tokenized_text = bpe.tokenize(new_text)\n",
    "decoded_text = bpe.decode(tokenized_text)\n",
    "print(f\"Encoded '{new_text}':\", encoded_text)\n",
    "print(f\"Tokenized '{new_text}':\", tokenized_text)\n",
    "print(f\"Decoded '{tokenized_text}: {decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad19cd69-b702-4e2d-8a23-6ea97eea15f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'sell': ['s', 'e', 'l', 'l', '</w>']\n",
      "Tokenized 'sell': [0, 21, 32, 32, 11]\n",
      "Decoded '[0, 21, 32, 32, 11]: sell'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new text\n",
    "new_text = \"sell\"\n",
    "encoded_text = bpe.encode(new_text)\n",
    "tokenized_text = bpe.tokenize(new_text)\n",
    "decoded_text = bpe.decode(tokenized_text)\n",
    "print(f\"Encoded '{new_text}':\", encoded_text)\n",
    "print(f\"Tokenized '{new_text}':\", tokenized_text)\n",
    "print(f\"Decoded '{tokenized_text}: {decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b63ac0f-b429-49af-8e43-4af191f374e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'bush': ['b', 'u', 's', 'h', '</w>']\n",
      "Tokenized 'bush': [34, 33, 0, 14, 11]\n",
      "Decoded '[34, 33, 0, 14, 11]: bush'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new text\n",
    "new_text = \"bush\"\n",
    "encoded_text = bpe.encode(new_text)\n",
    "tokenized_text = bpe.tokenize(new_text)\n",
    "decoded_text = bpe.decode(tokenized_text)\n",
    "print(f\"Encoded '{new_text}':\", encoded_text)\n",
    "print(f\"Tokenized '{new_text}':\", tokenized_text)\n",
    "print(f\"Decoded '{tokenized_text}: {decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f77159d5-70cf-4527-be69-14fb239708e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'shit': ['s', 'h', 'i', 't', '</w>']\n",
      "Tokenized 'shit': [0, 14, 8, 31, 11]\n",
      "Decoded '[0, 14, 8, 31, 11]: shit'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new text\n",
    "new_text = \"shit\"\n",
    "encoded_text = bpe.encode(new_text)\n",
    "tokenized_text = bpe.tokenize(new_text)\n",
    "decoded_text = bpe.decode(tokenized_text)\n",
    "print(f\"Encoded '{new_text}':\", encoded_text)\n",
    "print(f\"Tokenized '{new_text}':\", tokenized_text)\n",
    "print(f\"Decoded '{tokenized_text}: {decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0d873-0b9a-4915-8123-904e117e66fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
